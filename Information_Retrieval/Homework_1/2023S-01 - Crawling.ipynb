{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Auxillary resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from hashlib import md5\n",
    "\n",
    "def get_unique_name(url, base_name):\n",
    "    \"\"\"\n",
    "    Handles the problem with file rewriting\n",
    "    \"\"\"\n",
    "    hash = md5((url + base_name).encode(\"utf-8\")).hexdigest()\n",
    "    # cnt, suffix = 0, \"\"\n",
    "\n",
    "    # while os.path.exists(hash + base_name + suffix):\n",
    "    #     cnt += 1\n",
    "    #     suffix = f\" ({cnt})\"\n",
    "    return hash + base_name\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def wget(url):\n",
    "    try:\n",
    "        # allow redirects - in case file is relocated\n",
    "        resp = requests.get(url, allow_redirects=True)\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "    if not resp.ok:\n",
    "        return None, None\n",
    "\n",
    "    m = re.search(r\"https?:\\/\\/.*\\/([^?#\\/]*\\.[^?#\\/]+)?.*\", url)\n",
    "    filename = m.group(1)\n",
    "\n",
    "    if filename is None or filename == \"\":  # if filename is not recognized or recognized as empty\n",
    "        filename = \"index.html\"             # assume it is a html page\n",
    "\n",
    "    return filename, resp.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. [15] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        if not re.match(r\"https?:\\/\\/.*\\/.*\", url):\n",
    "            url += \"/\"\n",
    "\n",
    "        self.url = url\n",
    "        self.content = None\n",
    "        self._filename = None\n",
    "        \n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        self._filename, self.content = wget(self.url)\n",
    "        ok = self.content is not None\n",
    "        if ok:\n",
    "            self._filename = get_unique_name(self.url, self._filename)\n",
    "        return ok\n",
    "    \n",
    "    def persist(self):\n",
    "        if self.content is None:\n",
    "            raise ValueError(\"Invalid state: no content loaded\")\n",
    "        with open(self._filename, \"wb+\") as f:\n",
    "            f.write(self.content)\n",
    "\n",
    "            \n",
    "    def load(self):\n",
    "        if self._filename is None or not os.path.exists(self._filename):\n",
    "            return False\n",
    "        else:\n",
    "            try:\n",
    "                with open(self._filename, \"rb\") as f:\n",
    "                    self.content = f.read()\n",
    "            except (IOError, OSError):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_doc = Document(\"https://download.samplelib.com/mp3/sample-3s.mp3\")\n",
    "music_doc.get()\n",
    "assert doc.content, \"Document download failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. [10] Parse HTML\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "1. `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "2. `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "3. `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "    \n",
    "    def parse(self):\n",
    "        if self.content is None:\n",
    "            raise ValueError(\"Invalid state: document is not loaded\")\n",
    "        \n",
    "        soup = BeautifulSoup(self.content)\n",
    "\n",
    "        anchors = soup.select(\"a\")\n",
    "        self.anchors = []\n",
    "\n",
    "        for anchor in anchors:\n",
    "            if anchor.has_attr(\"href\"):\n",
    "                address = urllib.parse.urljoin(self.url, anchor.attrs[\"href\"])\n",
    "                if address.startswith(\"http\"):\n",
    "                    self.anchors.append((anchor.string, address,))\n",
    "        \n",
    "        self.text = soup.get_text(separator='\\n')\n",
    "        \n",
    "        images = soup.select(\"img\")\n",
    "        self.images = []\n",
    "        \n",
    "        for image in images:\n",
    "            if image.has_attr(\"src\"):\n",
    "                self.images.append(urllib.parse.urljoin(self.url, image.attrs[\"src\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About | Aleksandr Lobanov\n",
      "Achievements\n",
      " \n",
      "About\n",
      " \n",
      "Projects\n",
      " \n",
      "Aleksandr Lobanov\n",
      "Aleksandr\n",
      "Lobanov\n",
      "Full Stack Developer & ML Engineer\n",
      "Nice to meet you! I am Alex, a frontend and backend developer, and a machine learning engineer. I am doing my bachelor's degree in Applied Artificial Intelligence at Innopolis University. I have been working on various projects such are web applications, analytics services, servers and others for three years. I am quickly trained and adaptive person who is always searching for something new and exciting. Also, I love participating in different competitions and often take the lead in them.\n",
      "Curriculum Vitae\n",
      "СV\n",
      "Projects\n",
      "About\n",
      " \n",
      "Achievements\n",
      "Projects\n",
      "About\n",
      "Nice to meet you! I am Alex, a frontend and backend developer, and a machine learning engineer. I am doing my bachelor's degree in Applied Artificial Intelligence at Innopolis University. I have been working on various projects such are web applications, analytics services, servers and others for three years. I am quickly trained and adaptive person who is always searching for something new and exciting. Also, I love participating in different competitions and often take the lead in them.\n",
      "Curriculum Vitae\n",
      "СV\n",
      "Projects\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocument(\"https://alobanov.space\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
    "\n",
    "**Criteria to succeed in the task**: \n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained from inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url=None, doc=None):\n",
    "        if doc is not None:\n",
    "            self.doc = doc\n",
    "            return\n",
    "        \n",
    "        if url is None:\n",
    "            raise ValueError(\"Either url or doc argument should be specified\")\n",
    "\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "\n",
    "\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        result = sent_tokenize(self.doc.text)\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        words = sum([word_tokenize(sent) for sent in self.get_sentences()], list())\n",
    "        words = list(filter(lambda x: x not in [\",\", \".\", \"!\", \":\", \";\"], words))\n",
    "        words = list(map(lambda s: s.lower(), words))\n",
    "        return Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 44), ('в', 22), ('иннополис', 21), ('с', 13), ('университет', 12), ('на', 12), ('университета', 11), ('центр', 10), ('«', 10), ('»', 10)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. [15] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        level = {source}\n",
    "        visited = set(source)\n",
    "        for i in range(1 + depth):\n",
    "            next_level = set()\n",
    "            for link in level:\n",
    "                if link in visited: \n",
    "                    # Skip visited\n",
    "                    continue\n",
    "                visited.add(link)\n",
    "                try:\n",
    "                    document = HtmlDocument(link)\n",
    "                    document.get()\n",
    "                    document.parse()\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "                next_level = next_level.union(map(lambda x: x[1], document.anchors))\n",
    "                links = map(lambda x: HtmlDocument(x[1]), document.anchors)\n",
    "                yield HtmlDocumentTextData(doc=document)\n",
    "            level = next_level\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/en/\n",
      "346 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/\n",
      "355 distinct word(s) so far\n",
      "https://innopolis.university/en/ido/\n",
      "461 distinct word(s) so far\n",
      "https://media.innopolis.university/news/webinar-for-international-candidates-/\n",
      "740 distinct word(s) so far\n",
      "https://media.innopolis.university/news/registration-innopolis-open-2020/\n",
      "880 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/team-faculty2/\n",
      "912 distinct word(s) so far\n",
      "https://media.innopolis.university/news/innopolis-university-extends-international-application-deadline-/\n",
      "980 distinct word(s) so far\n",
      "https://t.me/universityinnopolis\n",
      "996 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/postgraduate-study/\n",
      "1115 distinct word(s) so far\n",
      "https://innopolis.university/en/contacts/\n",
      "1131 distinct word(s) so far\n",
      "https://media.innopolis.university/news/cyber-resilience-petrenko/\n",
      "1349 distinct word(s) so far\n",
      "https://media.innopolis.university/en\n",
      "1408 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/bachelor/\n",
      "1583 distinct word(s) so far\n",
      "https://www.youtube.com/user/InnopolisU\n",
      "1601 distinct word(s) so far\n",
      "https://apply.innopolis.ru/en/\n",
      "3640 distinct word(s) so far\n",
      "https://innopolis.university/en/internationalpartners/\n",
      "3807 distinct word(s) so far\n",
      "https://media.innopolis.university/en/events/\n",
      "3814 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/team-faculty/\n",
      "3857 distinct word(s) so far\n",
      "https://apply.innopolis.university/en\n",
      "4267 distinct word(s) so far\n",
      "https://media.innopolis.university/news/self-driven-school/\n",
      "4332 distinct word(s) so far\n",
      "https://innopolis.university/en/incomingstudents/\n",
      "4927 distinct word(s) so far\n",
      "https://minobrnauki.gov.ru/\n",
      "5162 distinct word(s) so far\n",
      "https://innopolis.university/en/nir2022/\n",
      "5366 distinct word(s) so far\n",
      "https://innopolis.university/en/international-relations-office/\n",
      "5586 distinct word(s) so far\n",
      "https://innopolis.university/en/board/\n",
      "5650 distinct word(s) so far\n",
      "https://innopolis.university/en/organizatsiya-i-provedenie-meropriyatiy/\n",
      "5824 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/education-academics/\n",
      "5825 distinct word(s) so far\n",
      "https://career.innopolis.university/konkursnyezayavkiprofessorskoprepodavatelskogosostava/\n",
      "5926 distinct word(s) so far\n",
      "https://career.innopolis.university/en/\n",
      "6011 distinct word(s) so far\n",
      "https://innopolis.university/en/outgoingstudents/\n",
      "6353 distinct word(s) so far\n",
      "https://innopolis.university/en/?special=Y\n",
      "6362 distinct word(s) so far\n",
      "https://innopolis.university/en/form/\n",
      "6466 distinct word(s) so far\n",
      "https://innopolis.university/en/proekty/activity/\n",
      "6509 distinct word(s) so far\n",
      "https://panoroo.com/virtual-tours/NvQZM6B2\n",
      "6512 distinct word(s) so far\n",
      "https://innopolis.university/lk/\n",
      "6519 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/techcenters/\n",
      "6521 distinct word(s) so far\n",
      "https://innopolis.university/\n",
      "6542 distinct word(s) so far\n",
      "https://dovuz.innopolis.university/\n",
      "6656 distinct word(s) so far\n",
      "https://innopolis.university/en/faculty/\n",
      "7248 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/stud-life/\n",
      "7304 distinct word(s) so far\n",
      "https://alumni.innopolis.university/\n",
      "7463 distinct word(s) so far\n",
      "https://innopolis.university/en/team/\n",
      "7463 distinct word(s) so far\n",
      "https://media.innopolis.university/en/news/\n",
      "7463 distinct word(s) so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "https://innopolis.university/search/\n",
      "7465 distinct word(s) so far\n",
      "https://vk.com/innopolisu\n",
      "7622 distinct word(s) so far\n",
      "https://innopolis.university/en/startupstudio/\n",
      "7660 distinct word(s) so far\n",
      "https://media.innopolis.university/news/devops-summer-school/\n",
      "7753 distinct word(s) so far\n",
      "https://media.innopolis.university/news/webinar-interstudents-eng/\n",
      "7759 distinct word(s) so far\n",
      "https://innopolis.university/en/about/\n",
      "7819 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/master/\n",
      "7834 distinct word(s) so far\n",
      "https://innopolis.university/en/research/\n",
      "7861 distinct word(s) so far\n",
      "https://media.innopolis.university/en/\n",
      "7861 distinct word(s) so far\n",
      "https://innopolis.university/en/campus\n",
      "7940 distinct word(s) so far\n",
      "https://career.innopolis.university/en/job/\n",
      "8269 distinct word(s) so far\n",
      "https://innopolis.university/en/teachingexcellencecenter/\n",
      "8421 distinct word(s) so far\n",
      "https://innopolis.university/en/writinghubhome/\n",
      "8430 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/\n",
      "8430 distinct word(s) so far\n",
      "https://media.innopolis.university/news/data-fest-2019/\n",
      "8467 distinct word(s) so far\n",
      "https://innopolis.university/en/ido/#block5358\n",
      "8467 distinct word(s) so far\n",
      "https://innopolis.university/antiterror/?lang=ru&id=12&site=s1&template=university24&landing_mode=edit\n",
      "8523 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-software-service-engineering/\n",
      "8606 distinct word(s) so far\n",
      "https://media.innopolis.university/news/road-map-robotics/\n",
      "8699 distinct word(s) so far\n",
      "https://minobrnauki.gov.ru/action/jhpolitika/\n",
      "8771 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/team-faculty/#block2651\n",
      "8771 distinct word(s) so far\n",
      "https://minobrnauki.gov.ru/press-center/gallery/\n",
      "8840 distinct word(s) so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/files/объявление_о_конкурсном_отборе_от_15_10_2020.pdf\n",
      "Skipping https://innopolis.university/files/объявление_о_конкурсном_отборе_от_15_10_2020.pdf\n",
      "https://vk.com/\n",
      "8849 distinct word(s) so far\n",
      "https://climate.enveco.es/\n",
      "9049 distinct word(s) so far\n",
      "https://drive.google.com/file/d/1_edC2ND8OrKBXL-G2bkxla178Gj9cjre/view\n",
      "9054 distinct word(s) so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/files/объявление о конкурсном отборе от 03.07.2020.pdf\n",
      "Skipping https://innopolis.university/files/объявление о конкурсном отборе от 03.07.2020.pdf\n",
      "https://minobrnauki.gov.ru/about/deps/\n",
      "9184 distinct word(s) so far\n",
      "https://sws.comp.nus.edu.sg/Topics%20and%20Clusters.htm\n",
      "9202 distinct word(s) so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/files/%D0%BE%D0%B1%D1%8A%D1%8F%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BE%20%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%BD%D0%BE%D0%BC%20%D0%BE%D1%82%D0%B1%D0%BE%D1%80%D0%B5%20%D0%BE%D1%82%2004.03.2022.pdf\n",
      "Skipping https://innopolis.university/files/%D0%BE%D0%B1%D1%8A%D1%8F%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BE%20%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%BD%D0%BE%D0%BC%20%D0%BE%D1%82%D0%B1%D0%BE%D1%80%D0%B5%20%D0%BE%D1%82%2004.03.2022.pdf\n",
      "https://innopolis.university/en/professor/rabab-marouf-/\n",
      "9203 distinct word(s) so far\n",
      "https://t.me/StudentAffairs_bot\n",
      "9208 distinct word(s) so far\n",
      "https://media.innopolis.university/news/Indie-GameDev-hack/\n",
      "9263 distinct word(s) so far\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '35e353644b3eb75bcb0a4084ee61f891%D0%BF%D1%80%D0%B8%D0%BA%D0%B0%D0%B7%20%D0%BE%D1%82%2008.12.2021%20%E2%84%96%20%D0%9E%D0%94_%D0%90%D0%94-01%20%D0%9E%20%D0%B2%D0%BD%D0%B5%D1%81%D0%B5%D0%BD%D0%B8%D0%B8%20%D0%B8%D0%B7%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9%20%D0%B2%20%D0%9F%D0%BE%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BE%20%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%BD%D0%BE%D0%B9%20%D0%BA%D0%BE%D0%BC%D0%B8%D1%81%D1%81%D0%B8%D0%B8%20%D0%B8%20%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BA%D0%B5%20%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%B0%20%D0%BD%D0%B0%20%D0%B7%D0%B0%D0%BC.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [84], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m counter \u001b[39m=\u001b[39m Counter()\n\u001b[0;32m      4\u001b[0m \u001b[39m# I have intentially changed depth from 2 to 1 since the number of links \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# that appears with depth=2 is too large to be proceed with my local machine\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m crawler\u001b[39m.\u001b[39mcrawl_generator(\u001b[39m\"\u001b[39m\u001b[39mhttps://innopolis.university/en/\u001b[39m\u001b[39m\"\u001b[39m, depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(c\u001b[39m.\u001b[39mdoc\u001b[39m.\u001b[39murl)\n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m c\u001b[39m.\u001b[39mdoc\u001b[39m.\u001b[39murl[\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m:] \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m.pdf\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.mp3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.avi\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.mp4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn [83], line 17\u001b[0m, in \u001b[0;36mCrawler.crawl_generator\u001b[1;34m(self, source, depth)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     document \u001b[39m=\u001b[39m HtmlDocument(link)\n\u001b[1;32m---> 17\u001b[0m     document\u001b[39m.\u001b[39;49mget()\n\u001b[0;32m     18\u001b[0m     document\u001b[39m.\u001b[39mparse()\n\u001b[0;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn [75], line 19\u001b[0m, in \u001b[0;36mDocument.get\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl)\n\u001b[0;32m     18\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpersist()\n",
      "Cell \u001b[1;32mIn [75], line 31\u001b[0m, in \u001b[0;36mDocument.persist\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid state: no content loaded\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_filename, \u001b[39m\"\u001b[39;49m\u001b[39mwb+\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     32\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\Aleksandr\\anaconda3\\envs\\mlenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '35e353644b3eb75bcb0a4084ee61f891%D0%BF%D1%80%D0%B8%D0%BA%D0%B0%D0%B7%20%D0%BE%D1%82%2008.12.2021%20%E2%84%96%20%D0%9E%D0%94_%D0%90%D0%94-01%20%D0%9E%20%D0%B2%D0%BD%D0%B5%D1%81%D0%B5%D0%BD%D0%B8%D0%B8%20%D0%B8%D0%B7%D0%BC%D0%B5%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9%20%D0%B2%20%D0%9F%D0%BE%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B5%20%D0%BE%20%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%BD%D0%BE%D0%B9%20%D0%BA%D0%BE%D0%BC%D0%B8%D1%81%D1%81%D0%B8%D0%B8%20%D0%B8%20%D0%BF%D0%BE%D1%80%D1%8F%D0%B4%D0%BA%D0%B5%20%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D0%B4%D0%B5%D0%BD%D0%B8%D1%8F%20%D0%BA%D0%BE%D0%BD%D0%BA%D1%83%D1%80%D1%81%D0%B0%20%D0%BD%D0%B0%20%D0%B7%D0%B0%D0%BC.pdf'"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "# I have intentially changed depth from 2 to 1 since the number of links \n",
    "# that appears with depth=2 is too large to be proceed with my local machine\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", depth=2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov  4 2022, 13:42:51) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "08a1b6d78b12a873d45f962f98da3008f45af31f504ce721cfef4bd13a47bdbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
